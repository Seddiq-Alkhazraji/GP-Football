{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811278db-0bcf-4124-a030-46de3430ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec68728-9e3f-4263-8ea6-a9431e593474",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data = pd.read_csv(\"C:/Users/ASUS/Desktop/players_integration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7803de1-9a54-4bd9-a51b-1157e944601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goalkeepers found: 1115\n",
      "Outfield players found: 9361\n",
      "Split and Drop complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = players_data.copy()\n",
    "\n",
    "# --- SPLIT GOALKEEPERS VS OUTFIELD ---\n",
    "mask_gk = df['main_pos'].str.contains('Goalkeeper', case=False, na=False) | df['Pos'].str.contains('GK', case=False, na=False)\n",
    "df_gk = df[mask_gk].copy()\n",
    "df_outfield = df[~mask_gk].copy()\n",
    "\n",
    "print(f\"Goalkeepers found: {len(df_gk)}\")\n",
    "print(f\"Outfield players found: {len(df_outfield)}\")\n",
    "\n",
    "# --- DEFINE CLEANING CONFIGURATION ---\n",
    "\n",
    "# 1. Metadata to Drop \n",
    "cols_metadata_drop = ['Born', 'Pos', 'Positions', 'Roles']\n",
    "\n",
    "# 2. Team Bias Metrics\n",
    "cols_team_bias_drop = [\n",
    "    'W', 'D', 'L', 'PPM_Team.Success', 'onG_Team.Success', 'onGA_Team.Success',\n",
    "    'plus_per__minus__Team.Success', 'plus_per__minus_90_Team.Success',\n",
    "    'On_minus_Off_Team.Success', 'onxG_Team.Success..xG.',\n",
    "    'onxGA_Team.Success..xG', 'On_minus_Off_Team.Success..xG',\n",
    "    'xGplus_per__minus__Team.Success..xG', 'xGplus_per__minus_90_Team.Success..xG'\n",
    "]\n",
    "\n",
    "# 3. GK Specific Keywords \n",
    "gk_keywords = ['_GK', 'Save_', 'GA', 'SoTA', 'CS', 'PKsv', 'PKA', 'Sweeper']\n",
    "\n",
    "# --- PROCESS OUTFIELD PLAYERS ---\n",
    "# Drop Metadata + Team Bias + GK Stats\n",
    "drop_outfield = cols_metadata_drop + cols_team_bias_drop + [c for c in df_outfield.columns if any(k in c for k in gk_keywords) and 'Att' not in c]\n",
    "df_outfield_clean = df_outfield.drop(columns=drop_outfield, errors='ignore')\n",
    "\n",
    "# --- PROCESS GOALKEEPERS ---\n",
    "# Drop Metadata + Team Bias ONLY (Keep GK Stats)\n",
    "drop_gk = cols_metadata_drop + cols_team_bias_drop\n",
    "df_gk_clean = df_gk.drop(columns=drop_gk, errors='ignore')\n",
    "\n",
    "print(\"Split and Drop complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4683f804-3682-47fe-9b8b-35bd923c75c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Outfielders ---\n",
      "Aggregating history for 8372 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\4199281315.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_weighted = df_input.groupby('Player', group_keys=False).apply(weighted_avg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 2543 unique player profiles.\n",
      "--- Processing Goalkeepers ---\n",
      "Aggregating history for 704 rows...\n",
      "Result: 242 unique player profiles.\n",
      "Outfield Cols: 200\n",
      "GK Cols: 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\4199281315.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_weighted = df_input.groupby('Player', group_keys=False).apply(weighted_avg)\n"
     ]
    }
   ],
   "source": [
    "def convert_to_per90(df_input, is_gk=False):\n",
    "    df_temp = df_input.copy()\n",
    "    \n",
    "    # Filter for valid minutes\n",
    "    df_temp = df_temp[df_temp['Mins_Per_90'] > 0]\n",
    "\n",
    "    # Standard Outfield Raw Stats\n",
    "    raw_stat_cols = [\n",
    "        'Cmp_Total', 'Att_Total', 'TotDist_Total', 'PrgDist_Total',\n",
    "        'Cmp_Short', 'Att_Short', 'Cmp_Medium', 'Att_Medium', 'Cmp_Long', 'Att_Long',\n",
    "        'Ast_Standard', 'KP', 'Final_Third', 'PPA', 'CrsPA', 'PrgP',\n",
    "        'Touches_Touches', 'Def Pen_Touches', 'Def 3rd_Touches', 'Mid 3rd_Touches', \n",
    "        'Att 3rd_Touches', 'Att Pen_Touches', 'Live_Touches',\n",
    "        'Att_Take', 'Succ_Take', 'Tkld_Take',\n",
    "        'Carries_Carries', 'TotDist_Carries', 'PrgDist_Carries', 'PrgC_Carries',\n",
    "        'Rec_Receiving', 'PrgR_Receiving',\n",
    "        'Gls_Standard', 'Sh_Standard', 'SoT_Standard',\n",
    "        'Tkl_Tackles', 'TklW_Tackles', 'Blocks_Blocks', 'Int_Def', 'Clr', 'Recov'\n",
    "    ]\n",
    "\n",
    "    # Add GK specific raw stats if this is the GK dataframe\n",
    "    if is_gk:\n",
    "        gk_cols = [c for c in df_temp.columns if any(k in c for k in gk_keywords)]\n",
    "        # Add them to the list to be converted\n",
    "        raw_stat_cols.extend(gk_cols)\n",
    "\n",
    "    # Execute Conversion\n",
    "    for col in raw_stat_cols:\n",
    "        if col in df_temp.columns:\n",
    "            # Check if numeric before dividing\n",
    "            if pd.api.types.is_numeric_dtype(df_temp[col]):\n",
    "                df_temp[col + '_Per90'] = df_temp[col] / df_temp['Mins_Per_90']\n",
    "                df_temp.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Fill categorical NaNs\n",
    "    cat_cols = ['secondary_pos_1', 'secondary_pos_2', 'role_1', 'role_2', 'role_3', 'role_4']\n",
    "    for col in cat_cols:\n",
    "        if col in df_temp.columns:\n",
    "            df_temp[col] = df_temp[col].fillna(\"None\")\n",
    "            \n",
    "    return df_temp\n",
    "\n",
    "# Apply to both\n",
    "df_outfield_final = convert_to_per90(df_outfield_clean, is_gk=False)\n",
    "df_gk_final = convert_to_per90(df_gk_clean, is_gk=True)\n",
    "\n",
    "# --- STEP 3: AGGREGATE SEASONAL HISTORY (WEIGHTED) ---\n",
    "\n",
    "def apply_season_weighting(df_input):\n",
    "    print(f\"Aggregating history for {len(df_input)} rows...\")\n",
    "    \n",
    "    # 1. Define Metadata Columns (Columns to KEEP from the LATEST season)\n",
    "    protected_cols = [\n",
    "        'Player', 'Team', 'Nation', 'Pos', 'main_pos', 'Url',\n",
    "        'secondary_pos_1', 'secondary_pos_2',\n",
    "        'role_1', 'role_2', 'role_3', 'role_4',\n",
    "        'Preferred foot', 'Season_End_Year', 'Age',\n",
    "        'Mins_Per_90', 'Min_Playing.Time', 'Starts_Starts', 'Subs_Subs', 'unSub_Subs'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # 2. Identify Stat Columns (The ones we will average)\n",
    "    stat_cols = [\n",
    "        c for c in df_input.columns \n",
    "        if pd.api.types.is_numeric_dtype(df_input[c]) \n",
    "        and c not in protected_cols\n",
    "    ]\n",
    "\n",
    "    DECAY_FACTOR = 0.75  # 2024=1.0, 2023=0.75, 2022=0.56...\n",
    "\n",
    "    # 3. Define the aggregation logic\n",
    "    def weighted_avg(group):\n",
    "        # Sort: Newest season first\n",
    "        group = group.sort_values('Season_End_Year', ascending=False)\n",
    "        \n",
    "        # Take the most recent metadata (Team, Age, Roles, etc.)\n",
    "        composite_player = group.iloc[0].copy()\n",
    "        \n",
    "        # If only 1 season, we are done\n",
    "        if len(group) == 1:\n",
    "            return composite_player\n",
    "        \n",
    "        # Calculate Weights based on how old the season is\n",
    "        latest_year = group['Season_End_Year'].max()\n",
    "        years_ago = latest_year - group['Season_End_Year']\n",
    "        weights = DECAY_FACTOR ** years_ago\n",
    "        \n",
    "        # Calculate Weighted Average for every stat column\n",
    "        for col in stat_cols:\n",
    "            # Only average if the column has data (not all NaNs)\n",
    "            if group[col].notna().any():\n",
    "                # Get values and valid weights\n",
    "                vals = group[col]\n",
    "                # Formula: Sum(Value * Weight) / Sum(Weights)\n",
    "                # Fill NaNs with 0 (here we assume 0 for missing stats in a season)\n",
    "                safe_vals = vals.fillna(0)\n",
    "                composite_player[col] = np.average(safe_vals, weights=weights)\n",
    "                \n",
    "        return composite_player\n",
    "\n",
    "    # 4. Apply GroupBy\n",
    "    # This collapses the dataframe: 10k rows -> 3k unique players\n",
    "    df_weighted = df_input.groupby('Player', group_keys=False).apply(weighted_avg)\n",
    "    \n",
    "    print(f\"Result: {len(df_weighted)} unique player profiles.\")\n",
    "    return df_weighted\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "# Apply to Outfielders\n",
    "print(\"--- Processing Outfielders ---\")\n",
    "df_outfield_aggregated = apply_season_weighting(df_outfield_final)\n",
    "\n",
    "# Apply to Goalkeepers\n",
    "print(\"--- Processing Goalkeepers ---\")\n",
    "df_gk_aggregated = apply_season_weighting(df_gk_final)\n",
    "\n",
    "\n",
    "print(f\"Outfield Cols: {df_outfield_final.shape[1]}\")\n",
    "print(f\"GK Cols: {df_gk_final.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902992ce-bd58-4d76-a639-b2f375d39876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating history for 8372 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\4199281315.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_weighted = df_input.groupby('Player', group_keys=False).apply(weighted_avg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 2543 unique player profiles.\n",
      "Aggregating history for 704 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\4199281315.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_weighted = df_input.groupby('Player', group_keys=False).apply(weighted_avg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 242 unique player profiles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\3069555990.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_proc[f'stats_embedding_{suffix}'] = df_proc.apply(get_vec, axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22624\\3069555990.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_proc[f'stats_embedding_{suffix}'] = df_proc.apply(get_vec, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Outfield Vector Length: 212\n",
      "Example GK Vector Length: 216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_embeddings(df_input, suffix=\"\"):\n",
    "    df_proc = df_input.copy()\n",
    "    \n",
    "    # 1. Fill NaNs\n",
    "    df_proc.fillna(0, inplace=True)\n",
    "    \n",
    "    # 2. Encode Roles\n",
    "    role_cols = ['role_1', 'role_2', 'role_3', 'role_4']\n",
    "    all_roles = set()\n",
    "    for col in role_cols:\n",
    "        if col in df_proc.columns:\n",
    "            unique_vals = df_proc[col].unique()\n",
    "            all_roles.update(unique_vals)\n",
    "            \n",
    "    for role in all_roles:\n",
    "        if role == 0 or role == \"\" or role == \"None\": continue\n",
    "        df_proc[f\"Role_{role}\"] = df_proc[role_cols].apply(lambda x: 1 if role in x.values else 0, axis=1)\n",
    "\n",
    "    # 3. Encode Preferred Foot \n",
    "    if 'Preferred foot' in df_proc.columns:\n",
    "        df_proc = pd.get_dummies(df_proc, columns=['Preferred foot'], prefix='Foot')\n",
    "        \n",
    "    # 4. Scale Numeric Metrics\n",
    "    # Exclude metadata and binary columns\n",
    "    time_cols = ['Mins_Per_90', 'Min_Playing.Time', 'Starts_Starts', 'Subs_Subs', 'unSub_Subs']\n",
    "    exclude = ['Player', 'Team', 'Nation', 'Pos', 'main_pos', 'secondary_pos_1', 'secondary_pos_2', 'Url', 'Age', 'Season_End_Year'] + role_cols + time_cols\n",
    "\n",
    "    # Select numeric columns\n",
    "    numeric_cols = df_proc.select_dtypes(include=['float64', 'int64', 'int32']).columns\n",
    "\n",
    "    # Filter out the excluded columns\n",
    "    numeric_cols = [c for c in numeric_cols if c not in exclude and not c.startswith('Role_') and not c.startswith('Foot_')]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_proc[numeric_cols] = scaler.fit_transform(df_proc[numeric_cols])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df_proc[numeric_cols] = scaler.fit_transform(df_proc[numeric_cols])\n",
    "    \n",
    "    # 5. Create Vector\n",
    "    # Ensure all features are float\n",
    "    feature_cols = list(numeric_cols) + [c for c in df_proc.columns if c.startswith('Role_') or c.startswith('Foot_')]\n",
    "    df_proc[feature_cols] = df_proc[feature_cols].astype(float)\n",
    "    \n",
    "    def get_vec(row):\n",
    "        return row[feature_cols].values.tolist()\n",
    "\n",
    "    df_proc[f'stats_embedding_{suffix}'] = df_proc.apply(get_vec, axis=1)\n",
    "    \n",
    "    return df_proc\n",
    "\n",
    "# Calculate weighted averages (Combines seasons)\n",
    "df_outfield_aggregated = apply_season_weighting(df_outfield_final)\n",
    "df_gk_aggregated = apply_season_weighting(df_gk_final)\n",
    "\n",
    "# Vectorize the AGGREGATED data\n",
    "df_outfield_vectorized = create_embeddings(df_outfield_aggregated, suffix=\"outfield\")\n",
    "df_gk_vectorized = create_embeddings(df_gk_aggregated, suffix=\"gk\")\n",
    "\n",
    "# Check results\n",
    "print(\"Example Outfield Vector Length:\", len(df_outfield_vectorized.iloc[0]['stats_embedding_outfield']))\n",
    "if not df_gk_vectorized.empty:\n",
    "    print(\"Example GK Vector Length:\", len(df_gk_vectorized.iloc[0]['stats_embedding_gk']))\n",
    "    \n",
    "# Save\n",
    "df_outfield_vectorized.to_csv('scout_outfield.csv', index=False)\n",
    "df_gk_vectorized.to_csv('scout_gk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181f04d-ac55-4c02-b330-17307ca5cd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d339a73-8f93-4b4a-85e4-94f07ce666ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
